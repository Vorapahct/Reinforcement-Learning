{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Taxi Example\n",
    "Code obtained from [here](https://www.kaggle.com/charel/learn-by-example-reinforcement-learning-with-gym). \n",
    "\n",
    "Taxi has to pickup a passenger and then drop them off at a target location. Episode ends when the taxi has dropped off a passenger. Walls exist to prevent the taxi from moving. Colours indicate different objects/objectives.\n",
    "\n",
    "### Rewards\n",
    "* -1 for each action\n",
    "* -10 for illegal action of pickup or dropoff\n",
    "* +20 for a successful delivery\n",
    "\n",
    "### Actions\n",
    "* 0: move south\n",
    "* 1: move north\n",
    "* 2: move east \n",
    "* 3: move west \n",
    "* 4: pickup passenger\n",
    "* 5: dropoff passenger\n",
    "\n",
    "### Rendering \n",
    "* blue: passenger\n",
    "* magenta: destination\n",
    "* yellow: empty taxi\n",
    "* green: full taxi\n",
    "* other letters: locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "from time import sleep\n",
    "from gym import envs\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use command gym.make(ENVIRONMENT_NAME) to pick the Gym environment to use. Reset() initializes with random state from the state space. Render() prints the current environment state. Action space is the number of actions that the agent can take and their type. In Gym, the observation space indicates the state space as well as type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Actions : 0...'5'\n",
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "print(env.action_space)\n",
    "print(\"Actions : 0...%a\" % str(env.action_space.n-1))\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of taking an action and the environment changing as a result. Action_space.sample() chooses a random action. Step(ACTION) applies the action to the environment and returns teh new state, reward, boolean indicating trial completion and dictionary object for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Start State\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "Reward: -24\n"
     ]
    }
   ],
   "source": [
    "rew_tot = 0\n",
    "obs = env.reset()\n",
    "print(\"Environment Start State\")\n",
    "env.render()\n",
    "for _ in range(6):\n",
    "    action = env.action_space.sample() \n",
    "    obs, rew, done, info = env.step(action) \n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "    \n",
    "print(\"Reward: %r\" % rew_tot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm\n",
    "This approach calculates the value of each state based on taking an action in a state. The function \"best_action_value\" sets the environment to the parameter state and then applies each action, returning the action with the best value, which is determined using the Bellman Equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action_value(s):\n",
    "    best_action = None\n",
    "    best_value  = float('-inf')\n",
    "\n",
    "    for a in range (0, NUM_ACTIONS):\n",
    "        env.env.s = s\n",
    "        s_new, reward, done, info = env.step(a) \n",
    "        v = reward + gamma * V[s_new]    \n",
    "        \n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_action = a\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V is an array that will hold the value of each state while $\\pi$ is the policy, that is an array that will take in a state and return the best action. Gamma is the discount factor and the significant_improvement value is used as a stopping factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "\n",
    "V = np.zeros([NUM_STATES]) \n",
    "Pi = np.zeros([NUM_STATES], dtype=int)  \n",
    "gamma = 0.9 \n",
    "significant_improvement = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Delta is used to calculate when to stop once it is smaller than the significant_improvement. Each iteration will calculate the value of each state (that is 500 states for the Taxi-V2 environment). Each state first has the best action selected, said best action is then taken and the environment updated, and finally the value is stored in the V array and $\\pi$ array has the best action stored for that state. After this is done for each state, that iteration is over. On the next iteration, the V is calculated using the old_v (previous iteration's V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41  iterations done\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    state = 0\n",
    "    \n",
    "    for s in range (0, NUM_STATES):\n",
    "        old_v = V[s]\n",
    "        action = best_action_value(s) \n",
    "        env.env.s = s  \n",
    "        s_new, rew, done, info = env.step(action) \n",
    "        V[s] = rew + gamma * V[s_new] \n",
    "        Pi[s] = action\n",
    "        delta = max(delta, np.abs(old_v - V[s]))\n",
    "        state += 1\n",
    "        \n",
    "    iteration += 1\n",
    "    if delta < significant_improvement:\n",
    "        print (iteration,' iterations done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "As the policy $\\pi$ has now been obtained, it is used to map actions onto the states and thus allow the agent to complete the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 8\n"
     ]
    }
   ],
   "source": [
    "rew_tot = 0\n",
    "obs  = env.reset()\n",
    "print(\"Starting state\")\n",
    "env.render()\n",
    "done = False\n",
    "\n",
    "while done != True: \n",
    "    action = Pi[obs]\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "    \n",
    "print(\"Reward: %r\" % rew_tot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
