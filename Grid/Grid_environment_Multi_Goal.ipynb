{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 4)]\n",
      "State Space:  (5, 5)\n",
      "[[-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. 50.]]\n",
      "Terminal State Co-ordinates [(4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "\n",
    "env_y = 5\n",
    "env_x = 5\n",
    "env_state_space = np.ones((env_y, env_x))\n",
    "env_state_space = np.negative(env_state_space)\n",
    "\n",
    "y_bound = env_y - 1\n",
    "x_bound = env_x - 1\n",
    "\n",
    "#env_term_state = [(0,0), (y_bound,0), (0,x_bound), (y_bound,x_bound)]\n",
    "\n",
    "env_term_state = [(y_bound,x_bound)]\n",
    "\n",
    "print(env_term_state)\n",
    "\n",
    "for item in env_term_state:\n",
    "    env_state_space[item] = 50\n",
    "\n",
    "print(\"State Space: \", env_state_space.shape)\n",
    "print(env_state_space)\n",
    "print(\"Terminal State Co-ordinates\", env_term_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movement Code of the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state,action):\n",
    "    # (y,x)\n",
    "    \n",
    "    switch = {\n",
    "        0: up,\n",
    "        1: right,\n",
    "        2: down,\n",
    "        3: left,\n",
    "        4: pickup\n",
    "    }\n",
    "\n",
    "    func = switch.get(action, lambda: \"Invalid action\")\n",
    "    new_state = func(state)\n",
    "    return new_state\n",
    "\n",
    "#make sure moves are within a bound\n",
    "def valid_move(state):\n",
    "    if state[0] > y_bound:\n",
    "        state = (state[0]-1, state[1])\n",
    "    elif state[0] < 0:\n",
    "        state = (state[0]+1, state[1])\n",
    "        \n",
    "    if state[1] > x_bound:\n",
    "        state = (state[0], state[1] - 1)\n",
    "    elif state[1] < 0:\n",
    "        state = (state[0], state[1] + 1)\n",
    "    \n",
    "    return state\n",
    "    \n",
    "def up(state):\n",
    "    new_state = (state[0] - 1, state[1])\n",
    "    new_state = valid_move(new_state)\n",
    "    return new_state\n",
    "\n",
    "def right(state):\n",
    "    new_state = (state[0], state[1] + 1)\n",
    "    new_state = valid_move(new_state)\n",
    "    return new_state\n",
    "    \n",
    "def down(state):\n",
    "    new_state = (state[0] + 1, state[1])\n",
    "    new_state = valid_move(new_state)\n",
    "    return new_state\n",
    "    \n",
    "def left(state):\n",
    "    new_state = (state[0], state[1] - 1)\n",
    "    new_state = valid_move(new_state)\n",
    "    return new_state\n",
    "\n",
    "#unused for now\n",
    "def pickup(state):\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual transition of the environment based on action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):        \n",
    "    \n",
    "    if state in env_term_state:\n",
    "        done = True\n",
    "        state_prime = state\n",
    "        reward = 0\n",
    "    else:\n",
    "        done = False\n",
    "        state_prime = move(state,action)\n",
    "        reward = env_state_space[(state_prime[0], state_prime[1])]\n",
    "        \n",
    "    return state_prime, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "[0, 1, 2, 3]\n",
      "['Up', 'Right', 'Down', 'Left']\n"
     ]
    }
   ],
   "source": [
    "ACTION_SPACE = list(range(0,4))\n",
    "\n",
    "print(\"Action Space:\")\n",
    "print(ACTION_SPACE)\n",
    "print([\"Up\", \"Right\", \"Down\", \"Left\"])\n",
    "\n",
    "Reward = env_state_space\n",
    "\n",
    "gamma = 1 \n",
    "significant_improvement = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "## _$Q^{new}$($s_{t}$, $a_{t}$)  &larr; (1 - $\\alpha$) $\\cdot$ Q($s_{t}$, $a_{t}$) + $\\alpha$ $\\cdot$ ($r_{t}$ + $\\gamma$ $\\cdot$ $max_{a}$Q($s_{t+1}$, $a$))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((len(ACTION_SPACE), env_y, env_x)) \n",
    "alpha = 0.000000000000001\n",
    "gamma = 0.6\n",
    "epsilon = 0.15\n",
    "\n",
    "for iteration in range(10000):\n",
    "    for y in range(env_y):\n",
    "        for x in range(env_x):\n",
    "            s = (y, x)      \n",
    "                    \n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(ACTION_SPACE)\n",
    "            else:\n",
    "                action = np.argmax(Q[:,y,x])\n",
    "\n",
    "            s_new, rew, done = step(s,action) \n",
    "            old_Q = Q[action][y][x]\n",
    "            \n",
    "            next_max = np.max(Q[:, y, x])\n",
    "            \n",
    "            #next_max = np.argwhere(Q[:,y,x] == np.max(Q[:, y, x]))\n",
    "            #next_max = random.choice(next_max)[0]\n",
    "\n",
    "            next_max = np.max(Q[:, y, x])\n",
    "            Q[action][y][x] = (1 - alpha) * old_Q + alpha * (rew + gamma * next_max)\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12192875 -0.12192854 -0.12192829 -0.12192787]\n"
     ]
    }
   ],
   "source": [
    "print(Q[:,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1\n",
      "Starting State:  (0, 4)\n",
      "Cumulative Reward:  -1.0\n",
      "Cumulative Reward:  -2.0\n",
      "Cumulative Reward:  -3.0\n",
      "Cumulative Reward:  -4.0\n",
      "Cumulative Reward:  -5.0\n",
      "Cumulative Reward:  -6.0\n",
      "Cumulative Reward:  -7.0\n",
      "Cumulative Reward:  -8.0\n",
      "Cumulative Reward:  -9.0\n",
      "Cumulative Reward:  -10.0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3b69820acab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cumulative Reward: \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreward_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-3b69820acab6>\u001b[0m in \u001b[0;36mrender\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "def render():\n",
    "    \n",
    "    screen.fill(BLACK)\n",
    "    \n",
    "    for y in range(height):\n",
    "        for x in range(width):                \n",
    "            if y == current_state[0] and x == current_state[1]:\n",
    "                colour = GREEN\n",
    "            elif env_state_space[y,x] != -1:\n",
    "                colour = RED\n",
    "            else:\n",
    "                colour = WHITE\n",
    "\n",
    "            rect_pos_x = x*(CONSTANT_SIZE+1)+1\n",
    "            rect_pos_y = y*(CONSTANT_SIZE+1)+1\n",
    "            rect = pygame.Rect(rect_pos_x, rect_pos_y , CONSTANT_SIZE, CONSTANT_SIZE)\n",
    "            pygame.draw.rect(screen, colour, rect)\n",
    "\n",
    "\n",
    "for episode in range(5):\n",
    "    \n",
    "    done = False\n",
    "    reward_total = 0\n",
    "\n",
    "    start_y = np.random.randint(low=0, high=env_y)\n",
    "    start_x = np.random.randint(low=0, high=env_x)\n",
    "    current_state = (start_y, start_x)\n",
    "\n",
    "    BLACK = (0, 0, 0)\n",
    "    WHITE = (255, 255, 255)\n",
    "    GREEN = (0, 255, 0)\n",
    "    RED = (255, 0, 0)\n",
    "\n",
    "    pygame.init()\n",
    "    \n",
    "    CONSTANT_SIZE = 50\n",
    "    height = env_state_space.shape[0]\n",
    "    width = env_state_space.shape[1]\n",
    "    \n",
    "    size_y = (CONSTANT_SIZE * height) + height + 1\n",
    "    size_x = (CONSTANT_SIZE * width) + width + 1\n",
    "    screen_size = (size_x, size_y)\n",
    "\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "\n",
    "    pygame.display.set_caption(\"Badworld\")\n",
    "\n",
    "    session = True\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "    wait_milli_sec = 350\n",
    "    \n",
    "    print(\"Episode \", episode + 1)\n",
    "    print(\"Starting State: \", current_state)\n",
    "\n",
    "    z = 0\n",
    "    \n",
    "    render()\n",
    "\n",
    "    pygame.display.flip()\n",
    "    pygame.time.wait(wait_milli_sec)    \n",
    "\n",
    "    while session:\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        \n",
    "        action = np.argmax(Q[:,current_state[0],current_state[1]])\n",
    "        \n",
    "        #action = Pi[current_state[0],current_state[1]]\n",
    "        obs, rew, done = step(current_state, action) \n",
    "        \n",
    "        current_state = obs\n",
    "        reward_total += rew\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "        render()\n",
    "        \n",
    "        print(\"Cumulative Reward: \" ,reward_total)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(wait_milli_sec)\n",
    "        clock.tick(60)\n",
    "        \n",
    "    print(\"Ending state: \", current_state)\n",
    "    \n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↑ ↑ → ↓ ← \n",
      "← ↑ → ↓ → \n",
      "→ ↓ → ← ↓ \n",
      "↓ ↓ → ↓ ↓ \n",
      "↑ → ← → x \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "switch_arrow = {\n",
    "    0: u\"\\u2191\",\n",
    "    1: u\"\\u2192\",\n",
    "    2: u\"\\u2193\",\n",
    "    3: u\"\\u2190\",\n",
    "    4: \"x\"\n",
    "}\n",
    "\n",
    "trajectory_grid = \"\"\n",
    "\n",
    "for y in range(env_y):\n",
    "    for x in range(env_x):\n",
    "        if((y,x) in env_term_state):\n",
    "            trajectory_grid += \"x \"\n",
    "        else:\n",
    "            trajectory_grid += switch_arrow.get(np.argmax(Q[:,y,x])) + \" \"\n",
    "    trajectory_grid += \"\\n\"\n",
    "    \n",
    "        \n",
    "print(trajectory_grid, \"\\n\")\n",
    "trajectory_grid = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "## _$Q^{new}$($s_{t}$, $a_{t}$)  &larr; (1 - $\\alpha$) $\\cdot$ Q($s_{t}$, $a_{t}$) + $\\alpha$ $\\cdot$ ($r_{t}$ + $\\gamma$ $\\cdot$ $max_{a}$Q($s_{t+1}$, $a$))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = np.zeros((len(ACTION_SPACE), env_y, env_x)) \n",
    "# alpha = 0.1\n",
    "# gamma = 0.6\n",
    "\n",
    "# for iteration in range(20):\n",
    "#     for y in range(env_y):\n",
    "#         for x in range(env_x):\n",
    "#             s = (y, x)\n",
    "#             for action in ACTION_SPACE:\n",
    "#                 s_new, rew, done = step(s,action) \n",
    "#                 old_Q = Q[action][y][x]\n",
    "#                 next_max = np.max(Q[:, y, x])\n",
    "#                 Q[action][y][x] = (1 - alpha) * old_Q + alpha * (rew + gamma * next_max)\n",
    "\n",
    "# print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "temp = np.zeros((1,3))\n",
    "temp[0][0] = 1\n",
    "temp[0][1] = 1 \n",
    "\n",
    "print(np.argwhere(temp == 0)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
