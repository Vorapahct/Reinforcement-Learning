{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "[(0, 0), (4, 0), (0, 4), (4, 4)]\n",
      "State Space:  (5, 5)\n",
      "[[50. -1. -1. -1. 50.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [50. -1. -1. -1. 50.]]\n",
      "Terminal State Co-ordinates [(0, 0), (4, 0), (0, 4), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "\n",
    "env_y = 5\n",
    "env_x = 5\n",
    "env_state_space = np.ones((env_y, env_x))\n",
    "env_state_space = np.negative(env_state_space)\n",
    "\n",
    "y_bound = env_y - 1\n",
    "x_bound = env_x - 1\n",
    "\n",
    "env_term_state = [(0,0), (y_bound,0), (0,x_bound), (y_bound,x_bound)]\n",
    "\n",
    "#env_term_state = [(y_bound,x_bound)]\n",
    "\n",
    "print(env_term_state)\n",
    "\n",
    "for item in env_term_state:\n",
    "    env_state_space[item] = 50\n",
    "\n",
    "print(\"State Space: \", env_state_space.shape)\n",
    "print(env_state_space)\n",
    "print(\"Terminal State Co-ordinates\", env_term_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movement Code of the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state,action):\n",
    "    # (y,x)\n",
    "    \n",
    "    switch = {\n",
    "        0: up,\n",
    "        1: right,\n",
    "        2: down,\n",
    "        3: left,\n",
    "        4: pickup\n",
    "    }\n",
    "\n",
    "    func = switch.get(action, lambda: \"Invalid action\")\n",
    "    new_state, penalty = func(state)\n",
    "    return new_state, penalty\n",
    "\n",
    "#make sure moves are within a bound\n",
    "def valid_move(state):\n",
    "    penalty = False\n",
    "    \n",
    "    if state[0] > y_bound:\n",
    "        state = (state[0]-1, state[1])\n",
    "        penalty = True\n",
    "    elif state[0] < 0:\n",
    "        state = (state[0]+1, state[1])\n",
    "        penalty = True\n",
    "        \n",
    "    if state[1] > x_bound:\n",
    "        state = (state[0], state[1] - 1)\n",
    "        penalty = True\n",
    "    elif state[1] < 0:\n",
    "        state = (state[0], state[1] + 1)\n",
    "        penalty = True\n",
    "    \n",
    "    return state, penalty\n",
    "    \n",
    "def up(state):\n",
    "    new_state = (state[0] - 1, state[1])\n",
    "    new_state, penalty = valid_move(new_state)\n",
    "    return new_state, penalty\n",
    "\n",
    "def right(state):\n",
    "    new_state = (state[0], state[1] + 1)\n",
    "    new_state, penalty = valid_move(new_state)\n",
    "    return new_state, penalty\n",
    "    \n",
    "def down(state):\n",
    "    new_state = (state[0] + 1, state[1])\n",
    "    new_state, penalty = valid_move(new_state)\n",
    "    return new_state, penalty\n",
    "    \n",
    "def left(state):\n",
    "    new_state = (state[0], state[1] - 1)\n",
    "    new_state, penalty = valid_move(new_state)\n",
    "    return new_state, penalty\n",
    "\n",
    "#unused for now\n",
    "def pickup(state):\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual transition of the environment based on action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):        \n",
    "    \n",
    "    if state in env_term_state:\n",
    "        done = True\n",
    "        state_prime = state\n",
    "        reward = 0\n",
    "    else:\n",
    "        done = False\n",
    "        state_prime, penalty = move(state,action)\n",
    "        if penalty == True:\n",
    "            add_reward = -50\n",
    "        else:\n",
    "            add_reward = 0\n",
    "        reward = env_state_space[(state_prime[0], state_prime[1])] + add_reward\n",
    "        \n",
    "    return state_prime, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "[0, 1, 2, 3]\n",
      "['Up', 'Right', 'Down', 'Left']\n"
     ]
    }
   ],
   "source": [
    "ACTION_SPACE = list(range(0,4))\n",
    "\n",
    "print(\"Action Space:\")\n",
    "print(ACTION_SPACE)\n",
    "print([\"Up\", \"Right\", \"Down\", \"Left\"])\n",
    "\n",
    "Reward = env_state_space\n",
    "\n",
    "gamma = 1 \n",
    "significant_improvement = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "## _$Q^{new}$($s_{t}$, $a_{t}$)  &larr; (1 - $\\alpha$) $\\cdot$ Q($s_{t}$, $a_{t}$) + $\\alpha$ $\\cdot$ ($r_{t}$ + $\\gamma$ $\\cdot$ $max_{a}$Q($s_{t+1}$, $a$))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00000000e+00 -1.85901204e+00 -1.90463973e+00 -1.74702991e+00\n",
      "    0.00000000e+00]\n",
      "  [ 4.98587711e+01  1.48032832e+00  3.02894445e-01  3.87379592e+01\n",
      "    4.33680061e+01]\n",
      "  [ 3.04065573e+01  3.03466662e+01  4.31418444e-02  1.60880395e+01\n",
      "    2.09196067e+01]\n",
      "  [ 4.45676145e-01  3.34582825e-01  5.36981671e-02  2.65534688e-01\n",
      "    6.25010835e-01]\n",
      "  [ 0.00000000e+00  1.58013165e-01  1.09710871e-01  3.10648736e-01\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  2.10749006e-01  2.75609686e+01  4.91718111e+01\n",
      "    0.00000000e+00]\n",
      "  [ 3.41725116e+00  1.11336129e+00  1.83081493e-01  1.47320271e+00\n",
      "   -2.37563502e+00]\n",
      "  [ 2.52160986e-02  6.65151380e-02  7.86755823e-02  1.00777507e-01\n",
      "   -2.73072950e+00]\n",
      "  [ 8.25159045e-01  6.34676103e-02  1.92720877e-01  2.00732663e+01\n",
      "   -1.09231727e+00]\n",
      "  [ 0.00000000e+00  1.69125455e-02  1.97143487e+01  4.34999828e+01\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  7.43871534e-01  3.64218842e-01  4.13272875e+00\n",
      "    0.00000000e+00]\n",
      "  [ 1.82267676e+00  7.47337726e-01 -1.45086011e-04  4.03904814e-01\n",
      "    4.49872395e-01]\n",
      "  [ 8.27305863e-01  1.30033613e-01  1.92756913e-03  1.54647955e-01\n",
      "    3.55181472e-01]\n",
      "  [ 4.72887071e+01  9.17793188e-01  1.64606750e-01  2.16020095e+00\n",
      "    4.34343260e+01]\n",
      "  [ 0.00000000e+00 -2.49926262e+00 -1.82869050e+00 -1.50502413e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  3.27693908e+01  1.61384115e-01  1.58410995e+00\n",
      "    0.00000000e+00]\n",
      "  [-1.67087271e+00  4.63887229e+01  2.24636758e+01  6.32923491e-01\n",
      "    1.98103979e+00]\n",
      "  [-1.08509739e+00  1.14468852e+00  9.44754500e+00 -6.40364814e-02\n",
      "    5.44331122e-02]\n",
      "  [-1.03982530e+00  3.33555095e+01  1.15568204e+01  2.48574382e-02\n",
      "    1.34622994e-01]\n",
      "  [ 0.00000000e+00  3.27693908e+01  3.32389201e-01  2.68072711e-01\n",
      "    0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((len(ACTION_SPACE), env_y, env_x)) \n",
    "alpha = 0.01\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "done = False\n",
    "\n",
    "for iteration in range(100):\n",
    "    for y in range(env_y):\n",
    "        for x in range(env_x):\n",
    "            s = (y, x)      \n",
    "\n",
    "            while done != True:\n",
    "\n",
    "                if random.uniform(0, 1) < epsilon:\n",
    "                    action = random.choice(ACTION_SPACE)\n",
    "                else:\n",
    "                    action = np.argmax(Q[:,s[0],s[1]])\n",
    "\n",
    "                s_new, rew, done = step(s,action) \n",
    "                old_Q = Q[action][s[0]][s[1]]\n",
    "\n",
    "                next_max = np.max(Q[:, s_new[0], s_new[1]])\n",
    "\n",
    "                Q[action][s[0]][s[1]] = (1 - alpha) * old_Q + alpha * (rew + gamma * next_max)\n",
    "                s = s_new\n",
    "\n",
    "            done = False\n",
    "                \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ← → → x \n",
      "↑ ← ← ↑ ↑ \n",
      "↑ ↑ ← ↑ ↑ \n",
      "↓ ← ← → ↓ \n",
      "x ← → → x \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "switch_arrow = {\n",
    "    0: u\"\\u2191\",\n",
    "    1: u\"\\u2192\",\n",
    "    2: u\"\\u2193\",\n",
    "    3: u\"\\u2190\",\n",
    "    4: \"x\"\n",
    "}\n",
    "\n",
    "trajectory_grid = \"\"\n",
    "\n",
    "for y in range(env_y):\n",
    "    for x in range(env_x):\n",
    "        if((y,x) in env_term_state):\n",
    "            trajectory_grid += \"x \"\n",
    "        else:\n",
    "            trajectory_grid += switch_arrow.get(np.argmax(Q[:,y,x])) + \" \"\n",
    "    trajectory_grid += \"\\n\"\n",
    "    \n",
    "        \n",
    "print(trajectory_grid, \"\\n\")\n",
    "trajectory_grid = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43.36800609 -2.37563502  0.4498724   1.98103979]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Q[:,1,4])\n",
    "print(np.argmax(Q[:,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1\n",
      "Starting State:  (3, 0)\n",
      "Cumulative Reward:  50.0\n",
      "Ending state:  (4, 0)\n",
      "Episode  2\n",
      "Starting State:  (3, 0)\n",
      "Cumulative Reward:  50.0\n",
      "Ending state:  (4, 0)\n",
      "Episode  3\n",
      "Starting State:  (3, 1)\n",
      "Cumulative Reward:  -1.0\n",
      "Cumulative Reward:  49.0\n",
      "Ending state:  (4, 0)\n",
      "Episode  4\n",
      "Starting State:  (1, 2)\n",
      "Cumulative Reward:  -1.0\n",
      "Cumulative Reward:  -2.0\n",
      "Cumulative Reward:  48.0\n",
      "Ending state:  (0, 0)\n",
      "Episode  5\n",
      "Starting State:  (2, 2)\n",
      "Cumulative Reward:  -1.0\n",
      "Cumulative Reward:  -2.0\n",
      "Cumulative Reward:  -3.0\n",
      "Cumulative Reward:  47.0\n",
      "Ending state:  (0, 0)\n"
     ]
    }
   ],
   "source": [
    "def render():\n",
    "    \n",
    "    screen.fill(BLACK)\n",
    "    \n",
    "    for y in range(height):\n",
    "        for x in range(width):                \n",
    "            if y == current_state[0] and x == current_state[1]:\n",
    "                colour = GREEN\n",
    "            elif env_state_space[y,x] != -1:\n",
    "                colour = RED\n",
    "            else:\n",
    "                colour = WHITE\n",
    "\n",
    "            rect_pos_x = x*(CONSTANT_SIZE+1)+1\n",
    "            rect_pos_y = y*(CONSTANT_SIZE+1)+1\n",
    "            rect = pygame.Rect(rect_pos_x, rect_pos_y , CONSTANT_SIZE, CONSTANT_SIZE)\n",
    "            pygame.draw.rect(screen, colour, rect)\n",
    "\n",
    "\n",
    "for episode in range(5):\n",
    "    \n",
    "    done = False\n",
    "    reward_total = 0\n",
    "\n",
    "    start_y = np.random.randint(low=0, high=env_y)\n",
    "    start_x = np.random.randint(low=0, high=env_x)\n",
    "    current_state = (start_y, start_x)\n",
    "\n",
    "    BLACK = (0, 0, 0)\n",
    "    WHITE = (255, 255, 255)\n",
    "    GREEN = (0, 255, 0)\n",
    "    RED = (255, 0, 0)\n",
    "\n",
    "    pygame.init()\n",
    "    \n",
    "    CONSTANT_SIZE = 50\n",
    "    height = env_state_space.shape[0]\n",
    "    width = env_state_space.shape[1]\n",
    "    \n",
    "    size_y = (CONSTANT_SIZE * height) + height + 1\n",
    "    size_x = (CONSTANT_SIZE * width) + width + 1\n",
    "    screen_size = (size_x, size_y)\n",
    "\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "\n",
    "    pygame.display.set_caption(\"Badworld\")\n",
    "\n",
    "    session = True\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "    wait_milli_sec = 350\n",
    "    \n",
    "    print(\"Episode \", episode + 1)\n",
    "    print(\"Starting State: \", current_state)\n",
    "\n",
    "    z = 0\n",
    "    \n",
    "    render()\n",
    "\n",
    "    pygame.display.flip()\n",
    "    pygame.time.wait(wait_milli_sec)    \n",
    "\n",
    "    while session:\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        \n",
    "        action = np.argmax(Q[:,current_state[0],current_state[1]])\n",
    "        \n",
    "        #action = Pi[current_state[0],current_state[1]]\n",
    "        obs, rew, done = step(current_state, action) \n",
    "        \n",
    "        current_state = obs\n",
    "        reward_total += rew\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "        render()\n",
    "        \n",
    "        print(\"Cumulative Reward: \" ,reward_total)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(wait_milli_sec)\n",
    "        clock.tick(60)\n",
    "        \n",
    "    print(\"Ending state: \", current_state)\n",
    "    \n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
